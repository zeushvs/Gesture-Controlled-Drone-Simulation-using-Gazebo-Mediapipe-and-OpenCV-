Drone Control System using Hand Gestures (Gazebo Simulation)

This project implements an intuitive drone navigation system controlled through hand gestures, simulated in Gazebo on Ubuntu Linux. The system integrates ROS (Robot Operating System) for communication and control, and employs Computer Vision (OpenCV) with Machine Learning for real-time gesture recognition.

Features

Hand gesture recognition using OpenCV and machine learning

ROS-based communication and control framework

Gesture-to-command mapping for drone navigation (takeoff, landing, directional movement)

Gazebo-based simulation for testing in a realistic 3D environment

Fully developed and tested on Ubuntu Linux

Tech Stack

ROS – Robot Operating System

Gazebo – 3D simulation environment

OpenCV – Computer vision for gesture recognition

Machine Learning – Gesture classification and control mapping

Ubuntu Linux – Development and execution platform

Outcome

The system successfully demonstrates drone navigation and control using natural hand gestures, eliminating the need for physical controllers. It highlights potential applications in human-robot interaction, AR/VR interfaces, and assistive robotics.
